{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-28T09:37:25.053549Z",
     "start_time": "2025-02-28T09:37:25.041076Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from pypdf import PdfReader \n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "import operator\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "embeddings = OpenAIEmbeddings()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now chain the prompt with the model and the output parser.\n",
    "\n",
    "<img src='images/chain2.png' width=\"1200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Paper content into pure texts\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Loading Vector A, English database "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# #Load Vector A\n",
    "# # creating a pdf reader object \n",
    "# paperName = \"Watermarking Diffusion Model\"\n",
    "# pdfFileName = f\"sample_papers/{paperName}.pdf\"\n",
    "# reader = PdfReader(pdfFileName) \n",
    "# # printing number of pages in pdf file \n",
    "# print(f\"Pages of the PDF are {len(reader.pages)}\") \n",
    "# \n",
    "# textFilePath = f\"paper_texts/text_of_{paperName}.txt\"\n",
    "# if not os.path.exists(textFilePath):\n",
    "#     for page in reader.pages:\n",
    "#         text = page.extract_text()\n",
    "#         with open(textFilePath, \"a\") as file:\n",
    "#                 file.write(text)\n",
    "# \n",
    "# \n",
    "# loader = TextLoader(textFilePath)\n",
    "# text_documents = loader.load()\n",
    "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "# documents = text_splitter.split_documents(text_documents)\n",
    "\n",
    "\n",
    "                \n",
    "def writePDFtoTextFile_foreign(_paperName):\n",
    "    _pdfFileName = f\"experiment/papers/foreign/{_paperName}.pdf\"\n",
    "    reader_ = PdfReader(_pdfFileName)\n",
    "    # printing number of pages in pdf file \n",
    "    print(f\"Pages of the PDF are {len(reader_.pages)}\")\n",
    "    textFilePath_ = f\"experiment/texts/foreign/{_paperName}.txt\"\n",
    "    if not os.path.exists(textFilePath_):\n",
    "        for page_1 in reader_.pages:\n",
    "            text_1 = page_1.extract_text()\n",
    "            with open(textFilePath_, \"a\") as file_1:\n",
    "                file_1.write(text_1)\n",
    "                \n",
    "def createVectorStore_foreign(_paperName):\n",
    "    filePath = f\"experiment/texts/foreign/{_paperName}.txt\"\n",
    "    loader_ = TextLoader(filePath)\n",
    "    text_documents_ = loader_.load()\n",
    "    print(f\"text_documents_ = {text_documents_}\")\n",
    "    text_splitter_ = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "    documents_ = text_splitter_.split_documents(text_documents_)\n",
    "    vectorstore_ = DocArrayInMemorySearch.from_documents(documents_, embeddings)\n",
    "    return vectorstore_\n",
    "\n",
    "            \n",
    "def writePDFtoTextFile_combined(_paperName, textFilePath_):\n",
    "    _pdfFileName = f\"experiment/papers/local_combine_english/{_paperName}.pdf\"\n",
    "    reader_ = PdfReader(_pdfFileName)\n",
    "    # printing number of pages in pdf file \n",
    "    print(f\"Pages of the PDF are {len(reader_.pages)}\")\n",
    "    for page_2 in reader_.pages:\n",
    "        text_2 = page_2.extract_text()\n",
    "        with open(textFilePath_, \"a\") as file_2:\n",
    "            file_2.write(text_2)\n",
    "            \n",
    "\n",
    "def writePDFtoTextFile(_paperName):\n",
    "    _pdfFileName = f\"experiment/papers/english/{_paperName}.pdf\"\n",
    "    reader_ = PdfReader(_pdfFileName)\n",
    "    # printing number of pages in pdf file \n",
    "    print(f\"Pages of the PDF are {len(reader_.pages)}\")\n",
    "    textFilePath_ = f\"experiment/texts/english/{_paperName}.txt\"\n",
    "    if not os.path.exists(textFilePath_):\n",
    "        for page_1 in reader_.pages:\n",
    "            text_1 = page_1.extract_text()\n",
    "            with open(textFilePath_, \"a\") as file_1:\n",
    "                file_1.write(text_1)\n",
    "\n",
    "\n",
    "def createVectorStore(_paperName):\n",
    "    filePath = f\"experiment/texts/english/{_paperName}.txt\"\n",
    "    loader_ = TextLoader(filePath)\n",
    "    text_documents_ = loader_.load()\n",
    "    text_splitter_ = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "    documents_ = text_splitter_.split_documents(text_documents_)\n",
    "    vectorstore_ = DocArrayInMemorySearch.from_documents(documents_, embeddings)\n",
    "    return vectorstore_\n",
    "\n",
    "def writeKeywordsDictToFile(path, dict):\n",
    "    with open(path, 'w') as f:  \n",
    "        for key, value in dict:  \n",
    "            f.write('%s:%s\\n' % (key, value))\n",
    "            \n",
    "\n",
    "def findCommonKeywordsForNewSinglePaper_english(_paperName, btw_local_before_filter, btw_combined_before_filter):\n",
    "    filePath = f\"experiment/texts/english/{_paperName}.txt\"\n",
    "    with open(filePath, 'r') as file_3:\n",
    "        text_new_paper = file_3.read().rstrip()\n",
    "        \n",
    "    # This is for English version\n",
    "    betweenness_english = btw.run(text_new_paper)\n",
    "    filtered_betweenness_sk = {}\n",
    "    for node in btw_combined_before_filter:\n",
    "        if node in btw_local_before_filter.keys() and node in betweenness_english.keys():\n",
    "            filtered_betweenness_sk[node] = btw_combined_before_filter[node]\n",
    "\n",
    "    sorted_keywords = sorted(filtered_betweenness_sk.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    writeKeywordsDictToFile(f\"experiment/keywords/common_with_{_paperName}.txt\", sorted_keywords)\n",
    "    \n",
    "\n",
    "def scalable_betweenness_centrality_for_each_english(englishPaper, btw_local_before_filter):\n",
    "    filePath = f\"experiment/texts/english/{englishPaper}.txt\"\n",
    "    with open(filePath, 'r') as file_4:\n",
    "        text_new_paper = file_4.read().rstrip()\n",
    "    \n",
    "    # This is new English paper\n",
    "    btw_new_english = btw.run(text_new_paper)\n",
    "    \n",
    "    # Compute SCB without re-run btw on combined graph\n",
    "    filtered_betweenness_sk = {}\n",
    "    for node in btw_new_english:\n",
    "        if node in btw_local_before_filter.keys():\n",
    "            filtered_betweenness_sk[node] = btw_new_english[node] + btw_local_before_filter[node]\n",
    "    \n",
    "    sorted_keywords = sorted(filtered_betweenness_sk.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    writeKeywordsDictToFile(f\"experiment/keywords/SCB/proposed_common_{englishPaper}.txt\", sorted_keywords)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-17T23:51:07.760786Z",
     "start_time": "2025-02-17T23:51:07.746528Z"
    }
   },
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setting up Pinecone\n",
    "\n",
    "Loading Vector B, Asian research database:\n",
    "\n",
    "So far we've used an in-memory vector store. In practice, we need a vector store that can handle large amounts of data and perform similarity searches at scale. For this example, we'll use [Pinecone](https://www.pinecone.io/).\n",
    "\n",
    "The first step is to create a Pinecone account, set up an index, get an API key, and set it as an environment variable `PINECONE_API_KEY`.\n",
    "\n",
    "Then, we can load the transcription documents into Pinecone:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "#Load Vector B\n",
    "b_paperNames = [\"‘Detention’_ A Clichéd Salute to Freedom - The News Lens International Edition\", \n",
    "                \"Cohen-Puppetry and the Destruction of the Object\", \n",
    "                \"Even the Bleakest Horror Games End More Hopefully than _Detention_ Does\", \n",
    "                \"Manning-Can the Avatar Speak \",\n",
    "                \"Performative Reckoning 240425\",\n",
    "                \"Psychological horror game Detention revisits 1960s Taiwan - Polygon\",\n",
    "                \"Review_ Detention (返校) _ New Bloom Magazine\",\n",
    "                \"Review_ In Taiwanese Horror Movie _Dete... Is The Real Monster _ Cinema Escapist\",\n",
    "                \"Roxworthy-Revitalizing Japanese American Internment\",\n",
    "                \"Schechner-Restoration of Behavior\",\n",
    "                \"Son-Performance of Care\",\n",
    "                \"Taylor-Acts of Transfer\",\n",
    "                \"Thiongo-EnactmentsPowerPolitics-1997\",\n",
    "                \"Tillis-The_Art_of_Puppetry_in_the_Age\",\n",
    "                \"Turner-Liminality and Communitas\",\n",
    "                \"Wu-Spectralizing the White Terror Horror Trauma and the Ghost Island Narrative in Detention\",\n",
    "                ]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-17T23:51:16.912547Z",
     "start_time": "2025-02-17T23:51:16.909025Z"
    }
   },
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pages of the PDF are 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 51 0 (offset 0)\n",
      "Ignoring wrong pointing object 76 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pages of the PDF are 10\n",
      "Pages of the PDF are 14\n",
      "Pages of the PDF are 17\n",
      "Pages of the PDF are 34\n",
      "Pages of the PDF are 5\n",
      "Pages of the PDF are 10\n",
      "Pages of the PDF are 7\n",
      "Pages of the PDF are 24\n",
      "Pages of the PDF are 101\n",
      "Pages of the PDF are 31\n",
      "Pages of the PDF are 27\n",
      "Pages of the PDF are 21\n",
      "Pages of the PDF are 8\n",
      "Pages of the PDF are 9\n",
      "Pages of the PDF are 15\n"
     ]
    }
   ],
   "source": [
    "\n",
    "textFilePath_b: str = \"\"\n",
    "for b_paperName in b_paperNames:\n",
    "    b_pdfFileName = f\"experiment/papers/local/{b_paperName}.pdf\"\n",
    "    reader = PdfReader(b_pdfFileName)\n",
    "    # printing number of pages in pdf file \n",
    "    print(f\"Pages of the PDF are {len(reader.pages)}\") \n",
    "    textFilePath_b = f\"experiment/texts/local_papers.txt\"\n",
    "    for page in reader.pages:\n",
    "        text = page.extract_text()\n",
    "        with open(textFilePath_b, \"a\") as file:\n",
    "                file.write(text)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-17T23:51:31.240723Z",
     "start_time": "2025-02-17T23:51:19.710011Z"
    }
   },
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for OpenAIEmbeddings\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass `openai_api_key` as a named parameter. (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValidationError\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 8\u001B[0m\n\u001B[1;32m      3\u001B[0m OPENAI_API_KEY \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mgetenv(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOPENAI_API_KEY\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m# OPENAI_API_KEY = \"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\"\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# OPENAI_API_KEY = \"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\"\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m# embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\u001B[39;00m\n\u001B[0;32m----> 8\u001B[0m embeddings \u001B[38;5;241m=\u001B[39m \u001B[43mOpenAIEmbeddings\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# index_name = \"research-paper-rag-index\"\u001B[39;00m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# \u001B[39;00m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;66;03m# pinecone = PineconeVectorStore.from_documents(\u001B[39;00m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m#     documents_b, embeddings, index_name=index_name\u001B[39;00m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# )\u001B[39;00m\n",
      "File \u001B[0;32m~/Workspace/rag/youtube-rag/.venv/lib/python3.9/site-packages/pydantic/main.py:341\u001B[0m, in \u001B[0;36mpydantic.main.BaseModel.__init__\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mValidationError\u001B[0m: 1 validation error for OpenAIEmbeddings\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass `openai_api_key` as a named parameter. (type=value_error)"
     ]
    }
   ],
   "source": [
    "\n",
    "# # use pinecone for first DB\n",
    "\n",
    "\n",
    "# embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "# embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# index_name = \"research-paper-rag-index\"\n",
    "# \n",
    "# pinecone = PineconeVectorStore.from_documents(\n",
    "#     documents_b, embeddings, index_name=index_name\n",
    "# )\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T19:32:46.147399Z",
     "start_time": "2024-05-05T19:32:46.004721Z"
    }
   },
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-29T00:26:31.487096Z",
     "start_time": "2024-04-29T00:26:31.479833Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import tiktoken\n",
    "\n",
    "\n",
    "# vectorstore4 = DocArrayInMemorySearch.from_documents(documents_b, embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# vectorstore4.similarity_search_with_score(\"White Terror\", k=10)[:10]"
   ],
   "metadata": {
    "collapsed": false
   },
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overwriting cache for 0 880\n",
      "Overwriting cache for 0 755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pages of the PDF are 29\n"
     ]
    }
   ],
   "source": [
    "# # Add one more vector C\n",
    "# c_paperNames = [\"Hassapopoulou-Playing with History_\"]\n",
    "# textFilePath_c: str = \"\"\n",
    "# for c_paperName in c_paperNames:\n",
    "#     c_pdfFileName = f\"sample_papers_asian/English/{c_paperName}.pdf\"\n",
    "#     reader = PdfReader(c_pdfFileName)\n",
    "#     # printing number of pages in pdf file \n",
    "#     print(f\"Pages of the PDF are {len(reader.pages)}\") \n",
    "#     textFilePath_c = f\"paper_texts_asian/text_of_all_papers_in_c.txt\"\n",
    "#     for page in reader.pages:\n",
    "#         text = page.extract_text()\n",
    "#         with open(textFilePath_c, \"a\") as file:\n",
    "#                 file.write(text)\n",
    "# \n",
    "# loader_c = TextLoader(textFilePath_c)\n",
    "# text_documents_c = loader_c.load()\n",
    "# text_splitter_c = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "# documents_c = text_splitter_c.split_documents(text_documents_c)\n",
    "# vectorstore5 = DocArrayInMemorySearch.from_documents(documents_c, embeddings)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-09T05:04:34.644917Z",
     "start_time": "2024-04-09T05:04:31.538583Z"
    }
   },
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Add one more vector D for Japanese and Chinese\n",
    "d_paperNames = [\"⑬文学部紀要 第49号_200-177\",\n",
    "                \"csat.org.tw_Journal.aspx_ID=22&ek=128&pg=1&d=3042\",\n",
    "                \"太平洋戦争期の布袋戯\",\n",
    "                # \"掌聲_作為文化外交工具的台灣布袋戲\",\n",
    "                \"書寫民族創傷\",\n",
    "                \"被動員的鄉土藝術 黃得時與太平洋戰爭期的布袋戲改造\",\n",
    "                \"返校：疊合慘白歲月的殘影，窺見戒嚴之下的人心｜端傳媒 Initium Media\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-17T23:51:51.057985Z",
     "start_time": "2025-02-17T23:51:51.055311Z"
    }
   },
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pages of the PDF are 29\n"
     ]
    }
   ],
   "source": [
    "\n",
    "d_paperName = d_paperNames[5]\n",
    "d_pdfFileName = f\"experiment/papers/foreign/{d_paperName}.pdf\"\n",
    "reader = PdfReader(d_pdfFileName)\n",
    "# printing number of pages in pdf file \n",
    "print(f\"Pages of the PDF are {len(reader.pages)}\") \n",
    "textFilePath_d = f\"experiment/texts/foreign/{d_paperName}.txt\"\n",
    "for page in reader.pages:\n",
    "    text = page.extract_text()\n",
    "    with open(textFilePath_d, \"a\") as file:\n",
    "            file.write(text)\n",
    "\n",
    "loader_d = TextLoader(textFilePath_d)\n",
    "text_documents_d = loader_d.load()\n",
    "text_splitter_d = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "documents_d = text_splitter_d.split_documents(text_documents_d)\n",
    "vectorstore_d = DocArrayInMemorySearch.from_documents(documents_d, embeddings)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-18T00:04:23.985713Z",
     "start_time": "2025-02-18T00:04:14.399603Z"
    }
   },
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-18T00:04:29.714230Z",
     "start_time": "2025-02-18T00:04:29.710242Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add one more vector E for Puppet in English\n",
    "# \"Ruizendaal-Puppets-identity-and-politics-in-Taiwan\"\n",
    "e_paperNames = [\"Bell-Puppets, Masks, and Performing Objects at the End of the Century\",\n",
    "                \"Bernstein-Dances with Things- Material Culture and the Performance of Race\",\n",
    "                \"Bird-David-Animism Revisited\",\n",
    "                \"Brown-thing-theory.2001\",\n",
    "                \"Goodall -Transferred Agencies- Performance and the Fear of Automatism\",\n",
    "                \"Kaplin-A Puppet Tree A Model for the Field of Puppet Theatre\",\n",
    "                \"Wu-Chapter 4 Religion and the Formation of Taiwanese Identities\",\n",
    "                \"Zamir-Puppets\"]"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 25 0 (offset 0)\n",
      "Ignoring wrong pointing object 27 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pages of the PDF are 14\n"
     ]
    },
    {
     "data": {
      "text/plain": "<langchain_community.vectorstores.docarray.in_memory.DocArrayInMemorySearch at 0x10cd51dc0>"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "e_paperName = e_paperNames[0]\n",
    "writePDFtoTextFile(e_paperName)\n",
    "\n",
    "createVectorStore(e_paperName)"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T00:04:47.465422Z",
     "start_time": "2025-02-18T00:04:45.984032Z"
    }
   },
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pages of the PDF are 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 51 0 (offset 0)\n",
      "Ignoring wrong pointing object 76 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pages of the PDF are 10\n",
      "Pages of the PDF are 14\n",
      "Pages of the PDF are 17\n",
      "Pages of the PDF are 34\n",
      "Pages of the PDF are 5\n",
      "Pages of the PDF are 10\n",
      "Pages of the PDF are 7\n",
      "Pages of the PDF are 24\n",
      "Pages of the PDF are 101\n",
      "Pages of the PDF are 31\n",
      "Pages of the PDF are 27\n",
      "Pages of the PDF are 21\n",
      "Pages of the PDF are 8\n",
      "Pages of the PDF are 9\n",
      "Pages of the PDF are 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 25 0 (offset 0)\n",
      "Ignoring wrong pointing object 27 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pages of the PDF are 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 9 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 35 0 (offset 0)\n",
      "Ignoring wrong pointing object 37 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pages of the PDF are 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 26 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pages of the PDF are 27\n",
      "Pages of the PDF are 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 24 0 (offset 0)\n",
      "Ignoring wrong pointing object 50 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pages of the PDF are 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 629 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pages of the PDF are 8\n",
      "Pages of the PDF are 301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 77 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pages of the PDF are 25\n"
     ]
    }
   ],
   "source": [
    "# Combined local B & new paper E to find common keywords\n",
    "common_paperNames = [\"‘Detention’_ A Clichéd Salute to Freedom - The News Lens International Edition\", \n",
    "                \"Cohen-Puppetry and the Destruction of the Object\", \n",
    "                \"Even the Bleakest Horror Games End More Hopefully than _Detention_ Does\", \n",
    "                \"Manning-Can the Avatar Speak \",\n",
    "                \"Performative Reckoning 240425.docx\",\n",
    "                \"Psychological horror game Detention revisits 1960s Taiwan - Polygon\",\n",
    "                \"Review_ Detention (返校) _ New Bloom Magazine\",\n",
    "                \"Review_ In Taiwanese Horror Movie _Dete... Is The Real Monster _ Cinema Escapist\",\n",
    "                \"Roxworthy-Revitalizing Japanese American Internment\",\n",
    "                \"Schechner-Restoration of Behavior\",\n",
    "                \"Son-Performance of Care\",\n",
    "                \"Taylor-Acts of Transfer\",\n",
    "                \"Thiongo-EnactmentsPowerPolitics-1997\",\n",
    "                \"Tillis-The_Art_of_Puppetry_in_the_Age\",\n",
    "                \"Turner-Liminality and Communitas\",\n",
    "                \"Wu-Spectralizing the White Terror Horror Trauma and the Ghost Island Narrative in Detention\",\n",
    "                \"Bell-Puppets, Masks, and Performing Objects at the End of the Century\",\n",
    "                \"Bernstein-Dances with Things- Material Culture and the Performance of Race\",\n",
    "                \"Bird-David-Animism Revisited\",\n",
    "                \"Brown-thing-theory.2001\",\n",
    "                \"Goodall -Transferred Agencies- Performance and the Fear of Automatism\",\n",
    "                \"Kaplin-A Puppet Tree A Model for the Field of Puppet Theatre\",\n",
    "                \"Wu-Chapter 4 Religion and the Formation of Taiwanese Identities\",\n",
    "                \"Zamir-Puppets\"]\n",
    "\n",
    "textFilePath_common: str = \"\"\n",
    "for common_paperName in common_paperNames:\n",
    "    common_pdfFileName = f\"experiment/papers/local_combine_english/{common_paperName}.pdf\"\n",
    "    reader = PdfReader(common_pdfFileName)\n",
    "    # printing number of pages in pdf file \n",
    "    print(f\"Pages of the PDF are {len(reader.pages)}\") \n",
    "    textFilePath_common = f\"experiment/texts/local_combine_english.txt\"\n",
    "    for page in reader.pages:\n",
    "        text = page.extract_text()\n",
    "        with open(textFilePath_common, \"a\") as file:\n",
    "                file.write(text)\n",
    "\n",
    "loader_common = TextLoader(textFilePath_common)\n",
    "text_documents_common = loader_common.load()\n",
    "text_splitter_common = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "documents_common = text_splitter_common.split_documents(text_documents_common)\n",
    "vectorstore_common = DocArrayInMemorySearch.from_documents(documents_common, embeddings)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-18T00:06:53.454472Z",
     "start_time": "2025-02-18T00:05:02.456548Z"
    }
   },
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Compute betweenness for local DB\n",
    "from keyword_alg.rake import Rake\n",
    "from keyword_alg.betweenness_centrality import Betweenness\n",
    "\n",
    "stoppath = \"FoxStoplist.txt\"\n",
    "\n",
    "with open(textFilePath_b, 'r') as file:\n",
    "    text_database = file.read().rstrip()\n",
    "\n",
    "\n",
    "btw = Betweenness(stoppath)\n",
    "# rake = Rake(stoppath)\n",
    "# rake_similarity = rake.run(text_database)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-18T00:06:53.619525Z",
     "start_time": "2025-02-18T00:06:53.455631Z"
    }
   },
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# To compute betweenness for local and combined DB, to re-use\n",
    "text_path_local_papers = f\"experiment/texts/local_papers.txt\"\n",
    "with open(text_path_local_papers, 'r') as file:\n",
    "    text_local_papers = file.read().rstrip()\n",
    "    \n",
    "btw_local_before_filter = btw.run(text_local_papers)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T06:58:15.520128Z",
     "start_time": "2024-05-03T06:53:28.185145Z"
    }
   },
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pages of the PDF are 14\n",
      "Pages of the PDF are 12\n",
      "Pages of the PDF are 10\n",
      "Pages of the PDF are 14\n",
      "Pages of the PDF are 17\n",
      "Pages of the PDF are 34\n",
      "Pages of the PDF are 5\n",
      "Pages of the PDF are 10\n",
      "Pages of the PDF are 7\n",
      "Pages of the PDF are 24\n",
      "Pages of the PDF are 101\n",
      "Pages of the PDF are 31\n",
      "Pages of the PDF are 27\n",
      "Pages of the PDF are 21\n",
      "Pages of the PDF are 8\n",
      "Pages of the PDF are 9\n",
      "Pages of the PDF are 15\n",
      "Pages of the PDF are 14\n",
      "Pages of the PDF are 29\n",
      "Pages of the PDF are 12\n",
      "Pages of the PDF are 10\n",
      "Pages of the PDF are 14\n",
      "Pages of the PDF are 17\n",
      "Pages of the PDF are 34\n",
      "Pages of the PDF are 5\n",
      "Pages of the PDF are 10\n",
      "Pages of the PDF are 7\n",
      "Pages of the PDF are 24\n",
      "Pages of the PDF are 101\n",
      "Pages of the PDF are 31\n",
      "Pages of the PDF are 27\n",
      "Pages of the PDF are 21\n",
      "Pages of the PDF are 8\n",
      "Pages of the PDF are 9\n",
      "Pages of the PDF are 15\n",
      "Pages of the PDF are 29\n",
      "Pages of the PDF are 27\n",
      "Pages of the PDF are 12\n",
      "Pages of the PDF are 10\n",
      "Pages of the PDF are 14\n",
      "Pages of the PDF are 17\n",
      "Pages of the PDF are 34\n",
      "Pages of the PDF are 5\n",
      "Pages of the PDF are 10\n",
      "Pages of the PDF are 7\n",
      "Pages of the PDF are 24\n",
      "Pages of the PDF are 101\n",
      "Pages of the PDF are 31\n",
      "Pages of the PDF are 27\n",
      "Pages of the PDF are 21\n",
      "Pages of the PDF are 8\n",
      "Pages of the PDF are 9\n",
      "Pages of the PDF are 15\n",
      "Pages of the PDF are 27\n",
      "Pages of the PDF are 24\n",
      "Pages of the PDF are 12\n",
      "Pages of the PDF are 10\n",
      "Pages of the PDF are 14\n",
      "Pages of the PDF are 17\n",
      "Pages of the PDF are 34\n",
      "Pages of the PDF are 5\n",
      "Pages of the PDF are 10\n",
      "Pages of the PDF are 7\n",
      "Pages of the PDF are 24\n",
      "Pages of the PDF are 101\n",
      "Pages of the PDF are 31\n",
      "Pages of the PDF are 27\n",
      "Pages of the PDF are 21\n",
      "Pages of the PDF are 8\n",
      "Pages of the PDF are 9\n",
      "Pages of the PDF are 15\n",
      "Pages of the PDF are 24\n",
      "Pages of the PDF are 14\n",
      "Pages of the PDF are 12\n",
      "Pages of the PDF are 10\n",
      "Pages of the PDF are 14\n",
      "Pages of the PDF are 17\n",
      "Pages of the PDF are 34\n",
      "Pages of the PDF are 5\n",
      "Pages of the PDF are 10\n",
      "Pages of the PDF are 7\n",
      "Pages of the PDF are 24\n",
      "Pages of the PDF are 101\n",
      "Pages of the PDF are 31\n",
      "Pages of the PDF are 27\n",
      "Pages of the PDF are 21\n",
      "Pages of the PDF are 8\n",
      "Pages of the PDF are 9\n",
      "Pages of the PDF are 15\n",
      "Pages of the PDF are 14\n",
      "Pages of the PDF are 8\n",
      "Pages of the PDF are 12\n",
      "Pages of the PDF are 10\n",
      "Pages of the PDF are 14\n",
      "Pages of the PDF are 17\n",
      "Pages of the PDF are 34\n",
      "Pages of the PDF are 5\n",
      "Pages of the PDF are 10\n",
      "Pages of the PDF are 7\n",
      "Pages of the PDF are 24\n",
      "Pages of the PDF are 101\n",
      "Pages of the PDF are 31\n",
      "Pages of the PDF are 27\n",
      "Pages of the PDF are 21\n",
      "Pages of the PDF are 8\n",
      "Pages of the PDF are 9\n",
      "Pages of the PDF are 15\n",
      "Pages of the PDF are 8\n",
      "Pages of the PDF are 301\n",
      "Pages of the PDF are 12\n",
      "Pages of the PDF are 10\n",
      "Pages of the PDF are 14\n",
      "Pages of the PDF are 17\n",
      "Pages of the PDF are 34\n",
      "Pages of the PDF are 5\n",
      "Pages of the PDF are 10\n",
      "Pages of the PDF are 7\n",
      "Pages of the PDF are 24\n",
      "Pages of the PDF are 101\n",
      "Pages of the PDF are 31\n",
      "Pages of the PDF are 27\n",
      "Pages of the PDF are 21\n",
      "Pages of the PDF are 8\n",
      "Pages of the PDF are 9\n",
      "Pages of the PDF are 15\n",
      "Pages of the PDF are 301\n",
      "Pages of the PDF are 25\n",
      "Pages of the PDF are 12\n",
      "Pages of the PDF are 10\n",
      "Pages of the PDF are 14\n",
      "Pages of the PDF are 17\n",
      "Pages of the PDF are 34\n",
      "Pages of the PDF are 5\n",
      "Pages of the PDF are 10\n",
      "Pages of the PDF are 7\n",
      "Pages of the PDF are 24\n",
      "Pages of the PDF are 101\n",
      "Pages of the PDF are 31\n",
      "Pages of the PDF are 27\n",
      "Pages of the PDF are 21\n",
      "Pages of the PDF are 8\n",
      "Pages of the PDF are 9\n",
      "Pages of the PDF are 15\n",
      "Pages of the PDF are 25\n"
     ]
    }
   ],
   "source": [
    "# Run full combined betweenness for each single new paper\n",
    "for newPaper in e_paperNames: #e_paperNames is all new papers\n",
    "    combinedPapers = [newPaper] + b_paperNames #b_paperNames is all local papers\n",
    "    textPath_common = f\"experiment/texts/local_combine_english/combined_with_{newPaper}.txt\"\n",
    "    for each in combinedPapers:\n",
    "        writePDFtoTextFile_combined(each, textPath_common)\n",
    "    \n",
    "    with open(textPath_common, 'r') as common_file:\n",
    "        text_combined = common_file.read().rstrip()\n",
    "        \n",
    "    btw_combined_before_filter = btw.run(text_combined)\n",
    "\n",
    "    # Find common keywords for each new English paper, and print to text file\n",
    "    writePDFtoTextFile(newPaper)\n",
    "    findCommonKeywordsForNewSinglePaper_english(newPaper, btw_local_before_filter, btw_combined_before_filter)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-30T08:53:09.489763Z",
     "start_time": "2024-04-30T07:21:49.562282Z"
    }
   },
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pages of the PDF are 14\n",
      "Pages of the PDF are 29\n",
      "Pages of the PDF are 27\n",
      "Pages of the PDF are 24\n",
      "Pages of the PDF are 14\n",
      "Pages of the PDF are 8\n",
      "Pages of the PDF are 301\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[17], line 6\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m paperName \u001B[38;5;129;01min\u001B[39;00m e_paperNames:\n\u001B[1;32m      5\u001B[0m     writePDFtoTextFile(paperName)\n\u001B[0;32m----> 6\u001B[0m     \u001B[43mfindCommonKeywordsForNewSinglePaper_english\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpaperName\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbtw_local_before_filter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbtw_combined_before_filter\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# This is for Foreign language version\u001B[39;00m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# betweenness_foreign = btw.run(text_database)\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[15], line 64\u001B[0m, in \u001B[0;36mfindCommonKeywordsForNewSinglePaper_english\u001B[0;34m(_paperName, btw_local_before_filter, btw_combined_before_filter)\u001B[0m\n\u001B[1;32m     61\u001B[0m     text_new_paper \u001B[38;5;241m=\u001B[39m file\u001B[38;5;241m.\u001B[39mread()\u001B[38;5;241m.\u001B[39mrstrip()\n\u001B[1;32m     63\u001B[0m \u001B[38;5;66;03m# This is for English version\u001B[39;00m\n\u001B[0;32m---> 64\u001B[0m betweenness_english \u001B[38;5;241m=\u001B[39m \u001B[43mbtw\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext_new_paper\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     65\u001B[0m filtered_betweenness_sk \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m     66\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m node \u001B[38;5;129;01min\u001B[39;00m betweenness_english:\n",
      "File \u001B[0;32m~/Workspace/rag/youtube-rag/keyword_alg/betweenness_centrality.py:172\u001B[0m, in \u001B[0;36mBetweenness.run\u001B[0;34m(self, text)\u001B[0m\n\u001B[1;32m    169\u001B[0m graph \u001B[38;5;241m=\u001B[39m calculate_word_graph(phrase_list)\n\u001B[1;32m    170\u001B[0m graph_reversed_weight \u001B[38;5;241m=\u001B[39m reverse_graph_edge_weight_for_shortest_path(graph)\n\u001B[0;32m--> 172\u001B[0m betweenness \u001B[38;5;241m=\u001B[39m \u001B[43mcalculate_betweenness\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgraph_reversed_weight\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    175\u001B[0m \u001B[38;5;66;03m# keyword_candidates = generate_candidate_keyword_scores(phrase_list, word_scores)\u001B[39;00m\n\u001B[1;32m    176\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m    177\u001B[0m \u001B[38;5;66;03m# sorted_keywords = sorted(keyword_candidates.iteritems(), key=operator.itemgetter(1), reverse=True)\u001B[39;00m\n\u001B[1;32m    178\u001B[0m \u001B[38;5;66;03m# return sorted_keywords\u001B[39;00m\n\u001B[1;32m    180\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m betweenness\n",
      "File \u001B[0;32m~/Workspace/rag/youtube-rag/keyword_alg/betweenness_centrality.py:142\u001B[0m, in \u001B[0;36mcalculate_betweenness\u001B[0;34m(G)\u001B[0m\n\u001B[1;32m    141\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcalculate_betweenness\u001B[39m(G):\n\u001B[0;32m--> 142\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mbetweenness_centrality\u001B[49m\u001B[43m(\u001B[49m\u001B[43mG\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mweight\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m<class 'networkx.utils.decorators.argmap'> compilation 4:4\u001B[0m, in \u001B[0;36margmap_betweenness_centrality_1\u001B[0;34m(G, k, normalized, weight, endpoints, seed, backend, **backend_kwargs)\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mcollections\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mgzip\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01minspect\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mitertools\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mre\u001B[39;00m\n",
      "File \u001B[0;32m~/Workspace/rag/youtube-rag/.venv/lib/python3.9/site-packages/networkx/utils/backends.py:412\u001B[0m, in \u001B[0;36m_dispatch.__call__\u001B[0;34m(self, backend, *args, **kwargs)\u001B[0m\n\u001B[1;32m    409\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m/\u001B[39m, \u001B[38;5;241m*\u001B[39margs, backend\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    410\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m backends:\n\u001B[1;32m    411\u001B[0m         \u001B[38;5;66;03m# Fast path if no backends are installed\u001B[39;00m\n\u001B[0;32m--> 412\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43morig_func\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    414\u001B[0m     \u001B[38;5;66;03m# Use `backend_name` in this function instead of `backend`\u001B[39;00m\n\u001B[1;32m    415\u001B[0m     backend_name \u001B[38;5;241m=\u001B[39m backend\n",
      "File \u001B[0;32m~/Workspace/rag/youtube-rag/.venv/lib/python3.9/site-packages/networkx/algorithms/centrality/betweenness.py:138\u001B[0m, in \u001B[0;36mbetweenness_centrality\u001B[0;34m(G, k, normalized, weight, endpoints, seed)\u001B[0m\n\u001B[1;32m    136\u001B[0m     S, P, sigma, _ \u001B[38;5;241m=\u001B[39m _single_source_shortest_path_basic(G, s)\n\u001B[1;32m    137\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:  \u001B[38;5;66;03m# use Dijkstra's algorithm\u001B[39;00m\n\u001B[0;32m--> 138\u001B[0m     S, P, sigma, _ \u001B[38;5;241m=\u001B[39m \u001B[43m_single_source_dijkstra_path_basic\u001B[49m\u001B[43m(\u001B[49m\u001B[43mG\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43ms\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    139\u001B[0m \u001B[38;5;66;03m# accumulation\u001B[39;00m\n\u001B[1;32m    140\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m endpoints:\n",
      "File \u001B[0;32m~/Workspace/rag/youtube-rag/.venv/lib/python3.9/site-packages/networkx/algorithms/centrality/betweenness.py:300\u001B[0m, in \u001B[0;36m_single_source_dijkstra_path_basic\u001B[0;34m(G, s, weight)\u001B[0m\n\u001B[1;32m    298\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m v \u001B[38;5;129;01min\u001B[39;00m D:\n\u001B[1;32m    299\u001B[0m     \u001B[38;5;28;01mcontinue\u001B[39;00m  \u001B[38;5;66;03m# already searched this node.\u001B[39;00m\n\u001B[0;32m--> 300\u001B[0m sigma[v] \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m sigma[pred]  \u001B[38;5;66;03m# count paths\u001B[39;00m\n\u001B[1;32m    301\u001B[0m S\u001B[38;5;241m.\u001B[39mappend(v)\n\u001B[1;32m    302\u001B[0m D[v] \u001B[38;5;241m=\u001B[39m dist\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# This is for Foreign language version\n",
    "# betweenness_foreign = btw.run(text_database)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-30T05:21:36.023169Z",
     "start_time": "2024-04-30T05:18:28.516543Z"
    }
   },
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pages of the PDF are 14\n",
      "Pages of the PDF are 29\n",
      "Pages of the PDF are 27\n",
      "Pages of the PDF are 24\n",
      "Pages of the PDF are 14\n",
      "Pages of the PDF are 8\n",
      "Pages of the PDF are 301\n",
      "Pages of the PDF are 25\n"
     ]
    }
   ],
   "source": [
    "#Run SCB by left and right Betweenness, without re-run btw on combined graph\n",
    "for newPaper in e_paperNames: #e_paperNames is all new papers\n",
    "    writePDFtoTextFile(newPaper)\n",
    "    btw_left = btw_local_before_filter\n",
    "    scalable_betweenness_centrality_for_each_english(newPaper, btw_left)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T08:26:25.638492Z",
     "start_time": "2024-05-03T08:16:29.375596Z"
    }
   },
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pages of the PDF are 12\n",
      "Pages of the PDF are 10\n",
      "Pages of the PDF are 14\n",
      "Pages of the PDF are 17\n",
      "Pages of the PDF are 34\n",
      "Pages of the PDF are 5\n",
      "Pages of the PDF are 10\n",
      "Pages of the PDF are 7\n",
      "Pages of the PDF are 24\n",
      "Pages of the PDF are 101\n",
      "Pages of the PDF are 31\n",
      "Pages of the PDF are 27\n",
      "Pages of the PDF are 21\n",
      "Pages of the PDF are 8\n",
      "Pages of the PDF are 9\n",
      "Pages of the PDF are 15\n"
     ]
    }
   ],
   "source": [
    "# Save all local PDF to texts individually\n",
    "for localPaper in b_paperNames: #e_paperNames is all new papers\n",
    "    # print each local paper to text file\n",
    "    _pdfFileName = f\"experiment/papers/local/{localPaper}.pdf\"\n",
    "    reader_ = PdfReader(_pdfFileName)\n",
    "    # printing number of pages in pdf file \n",
    "    print(f\"Pages of the PDF are {len(reader_.pages)}\")\n",
    "    textFilePath_ = f\"experiment/texts/local/{localPaper}.txt\"\n",
    "    if not os.path.exists(textFilePath_):\n",
    "        for page_1 in reader_.pages:\n",
    "            text_1 = page_1.extract_text()\n",
    "            with open(textFilePath_, \"a\") as file_1:\n",
    "                file_1.write(text_1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-02T07:59:13.363236Z",
     "start_time": "2024-05-02T07:59:12.723474Z"
    }
   },
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Get btw for local DB\n",
    "local_sorted_keywords = sorted(btw_local_before_filter.items(), key=operator.itemgetter(1), reverse=True)\n",
    "writeKeywordsDictToFile(f\"experiment/keywords/Local/all_local_papers_keywords.txt\", local_sorted_keywords)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-03T08:34:28.168173Z",
     "start_time": "2024-05-03T08:34:28.152752Z"
    }
   },
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "betweenness: [('taiwanese', 14472573.637999265), ('taiwan', 10061814.53833616), ('chinese', 5621453.449850505), ('temple', 4767348.268577458), ('puppet', 3788213.461776377), ('political', 3734066.197508117), ('national', 3691075.2295942893), ('de', 3141369.2931842785), ('identity', 3084995.7319417214), ('local', 2872393.6092100916), ('cultural', 2832196.5465179426), ('culture', 2584802.66288849), ('popular', 2540709.721788418), ('social', 2257036.838560225), ('people', 2255119.2878273046), ('performance', 2011221.3715311922), ('traditional', 1869514.9267999951), ('identities', 1582341.572775592), ('japanese', 1419628.054569085), ('own', 1278983.2159078866), ('china', 1113628.8966505998), ('li', 1058125.1968221643), ('society', 980739.6221852237), ('government', 978990.1246713124), ('historical', 896114.6325442572), ('modern', 884640.824841879), ('religious', 882393.7600941712), ('history', 816034.6736154166), ('ritual', 694877.2070148636), ('theater', 688109.6861825504), ('puppets', 663221.2703310284), ('en', 648223.6288142445), ('major', 634032.4375295329), ('chen', 611452.3186811914), ('life', 593930.3383088078), ('religion', 569538.2728403447), ('aboriginal', 542463.6517486199), ('theatre', 505520.3951132157), ('county', 497177.72228976706), ('official', 470214.78999650304), ('shi', 459554.22646667983), ('martial', 457050.84303522855), ('han', 440535.1210210959), ('themselves', 436530.24775172677), ('characters', 429632.5090975176), ('lin', 418653.9989008404), ('larger', 406057.37397750287), ('cult', 405857.9936708016), ('kmt', 404465.3698838301), ('form', 401362.71835539676), ('der', 392873.033495671), ('world', 390025.8016519463), ('status', 370614.8354607817), ('mainland', 354146.1134745107), ('stage', 351430.64141628717), ('huang', 348643.5419224705), ('performances', 344669.81644776935), ('main', 340291.81587876077), ('wu', 328971.1124143665), ('development', 305649.2387963173), ('late', 303245.0187090679), ('media', 289897.33066005714), ('time', 285552.9242169642), ('support', 284828.47547091875), ('president', 279434.16129263316), ('period', 277300.83681397967), ('regarding', 273800.5228863235), ('fujian', 272105.27569385245), ('arts', 265107.27517505747), ('chang', 258084.9834604516), ('hand', 256663.85783395107), ('usually', 252403.44870776532), ('context', 246748.31787561864), ('sometimes', 245598.46845324157), ('festivals', 242103.0192276432), ('language', 237915.2924439385), ('using', 231435.18276463766), ('recent', 228297.82279810045), ('forms', 228115.28671503882), ('highly', 224896.4237193341), ('outside', 218966.73265382307), ('image', 216062.9749154888), ('rule', 213352.49602698977), ('related', 210605.31086274548), ('defined', 204626.1191216146), ('series', 203572.3267912633), ('famous', 202802.65731808884), ('und', 201901.05714285714), ('tradition', 195042.05308441535), ('systems', 190843.51160318367), ('body', 190200.42208208542), ('war', 185047.1962835682), ('style', 183566.21329493486), ('account', 181650.4814972572), ('election', 176253.2665703353), ('produced', 176235.65807024814), ('company', 171297.70655178177), ('little', 171155.3684480779), ('southern', 168710.3576349723), ('art', 168547.82424846696), ('character', 167734.33418885752), ('sheng', 162778.46993058096), ('forces', 162352.04026496407), ('level', 162070.840321365), ('published', 156339.06287650054), ('ming', 151926.49768637307), ('magazine', 148833.28811110515), ('sources', 148753.17354758497), ('developed', 144669.3554275583), ('influenced', 143681.47299397134), ('ing', 143445.96185705264), ('television', 143007.33216089563), ('performing', 142960.92592130223), ('imperial', 142730.9809343433), ('actually', 139996.36949386002), ('stories', 139555.89008491515), ('elite', 137737.39646301448), ('followed', 137422.04100727735), ('ed', 135991.7326659469), ('pili', 135709.39164863012), ('international', 135402.55777735414), ('natural', 131520.43916121995), ('asia', 130365.01807432363), ('activities', 128693.74550865717), ('unique', 128175.67259094003), ('century', 126558.07309541041), ('closely', 125898.8826383344), ('publisher', 123620.63408605507), ('mass', 123440.33537990035), ('asian', 113977.10437071127), ('golden', 113942.9333333331), ('created', 112679.1485466361), ('established', 112501.96106324298), ('liu', 112216.96985930634), ('play', 111829.73404920202), ('effects', 110883.66220438859), ('re', 109638.03441493183), ('former', 109351.5185245314), ('able', 108747.4019061334), ('emerging', 108527.4792409066), ('performed', 105990.95450007601), ('view', 103539.41209855716), ('immigration', 103323.16126128017), ('huge', 103307.4168321899), ('settled', 101866.61948051982), ('finally', 98635.34268035895), ('urban', 98612.08577996072), ('indigenous', 98049.3608066553), ('notes', 97302.8035448214), ('chapter', 96868.85887862148), ('mainly', 96229.8536532914), ('start', 95507.74589169024), ('sinorama', 95314.63872549022), ('typical', 95102.40216658349), ('des', 95066.1), ('event', 95047.31785714287), ('masses', 94176.67004662006), ('fieldwork', 94124.06915489433), ('daily', 93425.341352398), ('elaborate', 92922.71163281212), ('communist', 92870.33392857188), ('short', 92692.7709124213), ('started', 91456.06860778114), ('written', 89481.94073328556), ('dpp', 89425.10787404315), ('family', 87844.7113691866), ('novels', 87479.0531746035), ('mountain', 86920.34659229705), ('shared', 85949.25976800972), ('follow', 85937.35588030606), ('success', 85665.09064738145), ('wachman', 85588.84354323334), ('bosco', 85571.64244701364), ('version', 84500.30646645011), ('civil', 84277.21646964153), ('dynasty', 84275.67945649281), ('situation', 84180.8716980151), ('taipei', 83810.88387445906), ('worship', 83726.9167866133), ('content', 83177.36966644537), ('den', 83073.62857142845), ('story', 82865.85965423436), ('candidate', 80371.02320179796), ('smoke', 79957.75135281359), ('light', 79261.80407043932), ('towards', 79049.31904761931), ('tv', 78952.60132725259), ('chiang', 78780.09156047211), ('named', 78543.88066378064), ('representative', 77513.78434938748), ('developments', 76581.49393939391), ('extensive', 75086.68732656223), ('politicians', 73894.85713143085), ('competing', 73861.83991430889), ('provided', 73322.39144978087), ('companies', 73290.70328282836), ('creation', 73096.46226551221), ('eternal', 71871.75834206668), ('quanzhou', 71598.36115958326), ('deeply', 71384.1523809528), ('conference', 71014.38827561324), ('performers', 70947.7545648784), ('living', 70395.07777777806), ('puppeteers', 70180.57823357207), ('considered', 69965.6278333187), ('commercial', 69677.56039280312), ('easily', 69632.83609446097), ('values', 69486.54367823421), ('expression', 67341.43976579023), ('extreme', 67249.64761904697), ('audience', 65474.5513518644), ('ancestral', 65081.426645590276), ('guo', 64635.60322551549), ('painted', 64573.75595238104), ('chain', 64154.320562923924), ('moment', 63882.36416561361), ('foreign', 63733.5353576977), ('puppeteer', 63653.03918948645), ('remembered', 62838.285981828565), ('express', 61912.203154973846), ('control', 61646.64366580051), ('perform', 60986.394254110586), ('hou', 60168.66536102785), ('hold', 59888.3108766234), ('opportunity', 59786.33571428571), ('south', 59651.25693526167), ('constant', 59553.55238095237), ('times', 59271.23075396818), ('example', 58633.7938410701), ('holding', 58616.34439114778), ('extended', 58535.396031745964), ('concepts', 57530.778013429364), ('music', 57082.9130109106), ('normal', 56905.08888888905), ('conduct', 56210.0273809524), ('james', 55866.6283244534), ('structures', 54936.11592728072), ('empire', 54420.28928571401), ('gained', 54106.40015540083), ('dragon', 53775.29596277915), ('fashion', 53728.06938894429), ('premier', 53559.964880952306), ('models', 53431.132840320905), ('male', 53310.8949675328), ('negative', 52893.97329921897), ('popularity', 51741.85280022248), ('creative', 51476.43122710622), ('qing', 51213.76388888903), ('presidential', 49868.964309987176), ('opera', 49782.02889610393), ('growth', 49725.51510989001), ('ma', 49439.575330879095), ('nationalist', 48707.12230547219), ('types', 48657.103014550674), ('bitter', 48086.2333333333), ('positive', 48013.8463955193), ('toys', 47675.875), ('symbol', 47402.68117864208), ('develop', 47286.553571428565), ('song', 47239.682815223954), ('opinion', 47160.50540011446), ('accompanied', 46995.76375985094), ('half', 46239.42817460317), ('hereafter', 46217.99200835396), ('continued', 45442.4825119322), ('pay', 45105.85924582642), ('carver', 44858.790189226806), ('true', 44518.188455988275), ('cloth', 44417.027182539554), ('plays', 44411.15243784081), ('ten', 44270.814145576645), ('whom', 42564.39228332939), ('drama', 41601.08625541126), ('law', 41525.87414529914), ('programs', 41195.88809523804), ('introduction', 40908.71786824279), ('tsai', 40647.78889076596), ('course', 40494.02457542458), ('office', 40481.74583333326), ('studio', 40324.099603174574), ('backstage', 39906.43095238094), ('transformation', 39374.23747758075), ('musicians', 39306.54523809525), ('roots', 39305.782448639744), ('enjoyed', 38984.032539682514), ('xie', 38889.448045010155), ('director', 38027.37460317449), ('yuan', 37439.649794413344), ('xu', 37218.633752358845), ('minor', 37169.12936507935), ('intense', 37009.13535561428), ('du', 36918.02749334004), ('imply', 36835.618462093495), ('base', 36816.05036540273), ('built', 36339.15484919678), ('beliefs', 36302.66666666664), ('exact', 36066.06381673881), ('academics', 36030.21666666667), ('thousand', 35879.440756465774), ('expel', 35793.23636363636), ('lu', 35778.66666666667), ('commitment', 35776.20277777776), ('associate', 35763.4), ('favorite', 35750.34285714286), ('dutch', 35710.52044622047), ('playing', 35136.44249639254), ('creating', 34807.31029411767), ('stages', 34427.560714285704), ('emergent', 33921.41168088431), ('eager', 33364.55476190521), ('ying', 33211.497765708606), ('gathered', 33199.49273088095), ('screen', 33131.0), ('phenomena', 32849.83047014264), ('information', 32592.12301587302), ('san', 32322.57590407961), ('province', 32144.759222721754), ('steady', 31574.81428571428), ('market', 31383.65541125588), ('moving', 30591.610443722828), ('due', 30297.074198124377), ('etc', 30250.971103896765), ('scholar', 29480.535208310295), ('xin', 29209.77202380978), ('attitude', 28623.181547619024), ('museum', 28113.691378066353), ('logical', 27622.86520562771), ('lazy', 26945.558248635287), ('post', 26890.266666666666), ('kai', 26775.06481990232), ('parties', 26772.47527472525), ('ceremonies', 26769.742857142875), ('children', 26527.505411255424), ('artist', 26101.143360806316), ('below', 25770.91220029973), ('beat', 25751.612121212125), ('openly', 25638.86937506928), ('brilliantly', 25630.860436785468), ('type', 25623.177960349156), ('document', 25530.5111832612), ('records', 25434.56470057721), ('largest', 24896.70476190473), ('represent', 24745.831746032094), ('industry', 24613.18479853479), ('dominated', 24451.11957140064), ('chu', 24295.845221434378), ('rubinstein', 24223.019047619047), ('singer', 24220.759523809553), ('gaining', 24186.925511988044), ('rigid', 24115.012990196083), ('ge', 24016.38849206348), ('belief', 24006.40714285715), ('array', 23938.89352425181), ('candidates', 23927.555501443007), ('peking', 23924.790476190487), ('representing', 23921.85), ('trying', 23896.186363636363), ('classes', 23881.352214452214), ('broken', 23876.8), ('preserve', 23875.775), ('paying', 23873.108088235294), ('innovation', 23872.85714285714), ('creativity', 23872.85714285714), ('reasons', 23872.345454545455), ('persons', 23871.0), ('tastes', 23868.347619047618), ('useless', 23862.266666666666), ('sea', 23860.114285714284), ('ent', 23854.11666666667), ('iron', 23851.86666666667), ('banning', 23851.01190476191), ('movies', 23847.50833333333), ('colonizers', 23844.91666666667), ('ad', 23843.9), ('follows', 23841.99220779221), ('resulted', 23836.051984126985), ('doubt', 23748.751211900882), ('exploding', 23714.381435231437), ('su', 23384.327102554307), ('broadcast', 22930.36094877348), ('ta', 22688.68756902459), ('macmillan', 22401.71976079512), ('film', 22048.089177489273), ('cartoon', 21771.576731601694), ('dress', 21546.949603174606), ('store', 20339.287486123558), ('vast', 20080.218061788593), ('xiao', 19469.938301015376), ('date', 18803.075747863266), ('heavily', 18743.559523809545), ('distinct', 18726.836890886876), ('centre', 18198.819937634547), ('legislative', 18166.169177472788), ('fantasy', 17738.53152958155), ('fist', 16698.742857142865), ('epic', 16202.520238095245), ('lady', 16015.416666666639), ('gu', 15953.827380952378), ('ultimate', 15851.434920634916), ('legislators', 15768.733549783556), ('marionette', 15691.40000000002), ('standard', 15640.27182305708), ('seven', 15635.994444444555), ('widespread', 15570.416810966728), ('depiction', 14945.200657838895), ('relates', 14898.22559523814), ('met', 14679.106349206351), ('taiyuan', 14314.283333333331), ('rallies', 14203.469200244175), ('bring', 14109.821804372194), ('shui', 13992.0112516656), ('aesthetic', 13930.709054834208), ('return', 13588.258333333328), ('multimedia', 13551.815476190497), ('creations', 13509.276984126838), ('inappropriate', 13402.623190742628), ('xing', 13365.133333333333), ('bad', 13260.818253968317), ('promotion', 13254.772258297238), ('1970s', 13242.645833333308), ('unprecedented', 13192.331364468855), ('planning', 12848.899999999987), ('glove', 12807.291322566321), ('-', 12788.566666666664), ('aforementioned', 12740.285714285701), ('love', 12653.86025641025), ('studios', 12652.174999999996), ('original', 12626.952380952393), ('cartoons', 12620.34960317452), ('hero', 12601.02023809523), ('february', 12587.8007936508), ('humans', 12550.686904761907), ('practitioners', 12526.560212418302), ('armed', 12487.853113553107), ('jade', 12470.333333333332), ('string', 12439.91666666666), ('hai', 12411.129020979037), ('pingdong', 12396.183333333318), ('speaking', 12352.804365079448), ('fuse', 12309.959523809524), ('confronted', 12303.991666666685), ('toy', 12296.287301587307), ('theatrical', 12274.684523809521), ('experiments', 12249.24826839827), ('crowd', 12160.690476190484), ('shuibian', 12153.336888315216), ('costume', 12120.833333333338), ('banned', 12091.83333333333), ('excellent', 12039.311904761906), ('manchu', 11984.0), ('shijie', 11979.54047619047), ('palgrave', 11976.25), ('broadcasts', 11974.0), ('bilingual', 11969.666666666666), ('monthly', 11969.666666666666), ('se', 11966.5), ('yunlin', 11966.0), ('tribal', 11966.0), ('dr', 11966.0), ('preference', 11966.0), ('televised', 11966.0), ('recorded', 11966.0), ('dynasties', 11966.0), ('teng', 7.5), ('frank', 5.0), ('hsieh', 5.0), ('photographed', 5.0), ('travels', 3.0), ('heads', 3.0), ('plastic', 3.0), ('wars', 3.0)]\n"
     ]
    }
   ],
   "source": [
    "# print(f'betweenness: {betweenness_english}')\n",
    "# print(f'rake similarity: {rake_similarity}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-29T00:45:38.101512Z",
     "start_time": "2024-04-29T00:45:38.097866Z"
    }
   },
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# print(f'betweenness: {betweenness_foreign}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's setup the new chain using Pinecone as the vector store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-18T00:07:43.067281Z",
     "start_time": "2025-02-18T00:07:43.049662Z"
    }
   },
   "outputs": [],
   "source": [
    "# chain = (\n",
    "#     {\"context\": pinecone.as_retriever(), \"question\": RunnablePassthrough()}\n",
    "#     | prompt\n",
    "#     | model\n",
    "#     | parser\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# This is the YouTube video we're going to use.\n",
    "YOUTUBE_VIDEO = \"https://www.youtube.com/watch?v=cdiD-9MMpb0\"\n",
    "\n",
    "model = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=\"gpt-3.5-turbo\")\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# chain = model | parser\n",
    "\n",
    "\n",
    "\n",
    "# If you can't answer the question, please explain the reason and show me the similarity among the question, the context_a and the context_b.\n",
    "\n",
    "# context_a below is written in English while context_b is in Japanese. Based on context_a and context_b, answer the question below. If you can't answer the question, please explain the reason and show me the similarity among the question, the context_a and the context_b.\n",
    "\n",
    "# Answer the question in Japanese based on the context in English below. In a second line, please translate your answer back in English."
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Local DB papers\n",
    "text_path_local_papers = f\"experiment/texts/local_papers.txt\"\n",
    "loader_local = TextLoader(text_path_local_papers)\n",
    "text_documents_local = loader_local.load()\n",
    "text_splitter_local = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "documents_local = text_splitter_local.split_documents(text_documents_local)\n",
    "vectorstore_local = DocArrayInMemorySearch.from_documents(documents_local, embeddings)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-18T00:07:35.120615Z",
     "start_time": "2025-02-18T00:07:19.011594Z"
    }
   },
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pages of the PDF are 24\n",
      "Pages of the PDF are 28\n",
      "Pages of the PDF are 11\n",
      "Pages of the PDF are 15\n",
      "Pages of the PDF are 119\n",
      "Pages of the PDF are 16\n",
      "Pages of the PDF are 25\n",
      "Pages of the PDF are 29\n"
     ]
    }
   ],
   "source": [
    "# Save all foreign papers and convert to text files\n",
    "for foreign_paper in d_paperNames:\n",
    "    writePDFtoTextFile_foreign(foreign_paper)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-05T22:38:38.135012Z",
     "start_time": "2024-05-05T22:38:33.732695Z"
    }
   },
   "execution_count": 63
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_documents_ = [Document(page_content='歷經近兩年的開發，以1960年代台灣戒嚴時期校園為題的恐怖遊戲《返校》，終於在1月13日於Steam上架，幾天內即衝上全球暢銷榜，得到來自世界各地玩家的一致好評。\\n《返校》（Detention ）\\n遊戲類型：暴力、血腥、冒險、獨立製作\\n發行日期: 2017年1月13日\\n開發商：赤燭遊戲\\n平台：Windows、Mac OS X、Steam OS\\n\\n在台北家中的我，也在剛發售時就買了。在陰森詭異的畫面、緊扣節奏的配樂下，我很快浸入了這個精心營造的小世界，一面品味着這森冷又親切的情調，一面讚賞着製作組編織劇情、統整風格的能力。當畫面指示女主角屏住呼吸，悄悄從扒着腳尾飯（又稱貢飯、拜飯）的魍魎身邊走過去的時候，我也──等等，我幹嗎跟着憋氣？於是我故意保持着正常呼吸走過去了。\\n大概恐怖遊戲和恐怖片一樣，有兩種觀賞方法，一種是入戲，一種是出戲。入戲是將自己代入主角的世界，與他/她一體同觀；出戲則是和製作組較勁，笑看你怎麼嚇我，留着心眼檢驗劇情的合理性。我的習慣是偏向出戲的，那些鬼哭、喘氣和幾處jump scare（突然出現的高能恐怖場景），我大抵都能淡然處之，但即便如此，當我玩到後期時也漸漸入戲了。\\n返校\\n《返校》遊戲截圖\\n我很慶幸，它並不只是一部刺激感官的鬼片。製作組很清楚，真正的恐怖來自人心，以及悲劇過後的淒涼。\\n遊戲前半段，每一個驚悚的地方，到後面都會給玩家一個合理的解釋，而且這解釋並不是製作組強塞給你的，而是你透過民俗常識推論出來的。到了遊戲後半段，也不再有故意嚇人以吸引注意力的地方，而是讓你一步一步更深入了解主角的故事，浸入那戒嚴時期白色恐怖的高壓氛圍之中。幾個小人物的幽靈、台灣民俗、校園與家庭的殘像，疊合成一幅無可奈何、亦無言以對的景觀，也疊合到我們的片斷記憶與歷史認知上。\\n（提示：下文有劇透，建議遊玩之後再閱讀。這款遊戲並不長，幾個小時即可玩過一遍）\\n用小成本講一個好故事\\n《返校》一開始，玩家對故事一無所知。序章之後，視角莫名地轉到女主角身上，場景也愈發詭異。一路走下去，藉由各種線索和回憶片段，玩家才會漸漸拼湊出劇情梗概：\\n家庭破碎的高中少女方芮欣，與輔導老師張明暉發生了戀情。張明暉的同事、和他一起辦讀書會的殷翠涵老師得知後，對他進行了勸阻。警告之下，他疏遠了方芮欣，希望讓她遠離是非。\\n返校\\n《返校》遊戲截圖\\n但方芮欣卻偷聽到了兩位老師的談話，在誤會、妒恨與渴望之下，方從對她有好感的魏仲廷學弟要到了讀書會的書單，並向教官舉報了殷翠涵，就像她媽媽舉報她爸爸貪污一樣，以為這樣可以把張老師搶回來。\\n孰料之後張明暉被判死刑，殷翠涵則流亡海外死不得歸，魏仲廷也被判十五年徒刑，還有許多參加讀書會的同學受到牽連。\\n方芮欣最後在無限悔恨中自殺，亡魂繫於校園不得超生，如陷夢魘地不斷重覆體驗着生前的罪過。\\n《返校》製作人姚舜庭表示，最開始是想做一款反烏托邦遊戲，然後覺得1960年代白色恐怖之下的台灣很適合作爲背景，便以此切入，但遊戲不會特別以具體歷史事件或人物為原型，而只呈現出這種環境底下可能發生的故事。\\n如果你想要藉這款遊戲來做一些政治方面的文章，例如踩踩國民黨、順便諷刺共產黨，你可能會對姚舜庭的說法略有失望。但我們應該關注的，並不是什麼黨派傾向，而是看這款遊戲怎麼說故事、有沒有把故事說好。\\n返校\\n《返校》遊戲截圖\\n《返校》將故事主軸放在「家庭破碎的懷春少女」方芮欣身上，她並不是一個特別有政治意識的人，只是和其他人一樣受着時代的重壓，一樣感到苦悶。張老師組織讀書會，在苦悶中尋找光明，而又與女主角同病相憐、異性相吸，這便是一個經典的「革命與愛情」的故事。然而他們還遠遠沒有到革命，遊戲中甚至根本沒有透露他們讀了什麼書，連他們的黨派傾向一點也未交代，就讓情治機關的扳機斷送了這些年輕的生命。或許，編劇正是因為不想被拉進無休止的政治鬥爭之中，所以在構思劇情時故意沒有交代讀書會的傾向與其他細節。\\n但《返校》確實在不長的篇幅裏面，把這個故事說好了。開發成本有限，故事的場地也不能太大，那就放在學校；需要變化，那就呈現前後幾十年的學校，再加上一間宮廟，因為需要宗教和靈異的元素……這樣條列下去，我們可以逐項分析出製作組為什麼這樣做，又為什麼不那麼做，然而這並沒有太大意義，因爲最重要的問題是遊戲的「統整性」。\\n返校\\n《返校》遊戲截圖\\n劇本、程式、繪畫、音樂，還有鬼怪、宮廟、偶戲、白色恐怖……等等元素，如何冶於一爐？這是最難的，而赤燭做到了。女主角的亡魂，民間信仰的鬼怪與城隍府，1960年代冷冰冰的校舍，以及如今已然廢棄、滿牆塗鴉的校舍；1960年代的家庭擺設、手搖式電話、播音設備、小豬撲……一個個存在你兒時記憶中，或僅僅只聽說過的舊物，都在《返校》裏一一再現。這些豐富的細節，讓主線劇情更為可信，對外國人來說也是一連串的有趣的文化衝擊。做這些、做好這些，實在比聚焦在政治鬥爭有意思多了。如果腦子裏只想着反國民黨或反反國民黨，那實在會錯過太多《返校》所能給我們創作同仁的啟發。\\n幽明疊合的場景，交互掩映的敘事\\n「疊合」是解讀《返校》伏筆的關鍵概念。序章還很平常的1960年代翠華中學校舍，為什麼到第一章開始，就到處貼滿了淨符、封條、符咒？\\n我推測：是因為學校後來鬧過鬼。鬧什麼鬼？就是懷着無限悔恨自殺的方芮欣，或許還有其他冤死的靈魂。\\n這時間線好像有點不對？如果按常理來說，是不對，可是通關一遍以後，我們就會明白，女主角是困在這學校幾十年的孤魂，她無限輪迴，反覆回憶生前的一切，然後只要有一點沒能真誠面對自己，就又要重來一遍──這也是為什麼遊戲標題畫面中，會以「初始新生」這個詞來代表「遊戲開始」。\\n返校\\n遊戲後半，你會看到遊戲前半的自己的殘像，這進一步暗示了這個空間是古今疊合的。\\n劇中的場景，其實都是孤魂方芮欣的心象世界，所以在這幾十年無數次的輪迴中，生前的學校、死後的學校、陽間的樓房、陰間的城隍府，甚至是若干次輪迴中的「自己」，全都疊在一起了。\\n也許，在最初幾次輪迴中，學校師生被暗夜出沒的方芮欣嚇到了，於是請來道士作法、畫符，然後貼上符咒的校舍就出現在了之後的輪迴中。但這既是方芮欣的心象世界，她自然就可以生出一些鑰匙、道具來協助通過障礙。這便能解釋，為什麼劇情到某處，某些場景就會突然就出現某些東西。\\n遊戲後半，你會看到遊戲前半的自己的殘像，這進一步暗示了遊戲中的時空是疊合的。\\n在這樣的設計下，遊戲中的種種謎題也就有了一個合理的解釋──每個謎題都象徵着回憶中的一重心理障礙，解謎的過程與道具，也都緊緊牽連着方芮欣的罪孽、創傷與懲罰，如黑白無常手中的令牌羽扇，如軍警與犯人造型的布袋戲偶，如歪斜顛倒的父母照片與時鐘謎題，或如最令人凜然、形狀是一把手槍的「書單」。\\n遊戲進行到後期，方芮欣的回憶漸漸完整，這時一個與主角輪廓一樣的黑影出現了；接觸她，場景頓時從比較接近現今的彩色，轉成黑白的當年，而玩家操控的角色也轉換成黑影──當年的方芮欣。\\n然後，黑影會就當年的情況，要你說出當時心中真實的想法。如果你答錯問題，遊戲就會進入結局之一：再度自殺、再次輪迴；而你如果全都答對，則代表這個孤魂終於能誠實面對過去的自己，才會出現另一個好結局：\\n步入中年的魏仲廷，再度返回即將拆除的母校，憑弔當年。當他回以前教室裏自己的座位上，方芮欣的幽影悄然浮現。人鬼相對，並無一語，畫面轉至製作組名單，終曲響起。\\n網上有玩家認為：方芮欣見到了未死的魏，或許終於可以解脫了吧？但製作組並沒有這樣演。魏仲廷雖可以渡過餘生，但我們不知道方芮欣是否真正解脫。如此留白，使劇情餘味不盡。\\n返校\\n遊戲進行到後期，方芮欣的回憶漸漸完整，這時一個與主角輪廓一樣的黑影出現了\\n《返校》所使用的這些交錯掩映的敘事手法並非首創，但它將之做到了非常圓融合理，與整個遊戲機制渾然一體。\\n如果我們把電子遊戲視為一種藝術形式，將這一類的冒險解謎遊戲視為一種文體，我們可以說，《返校》的演出是堪稱典範的。玩第二遍時，你往往能在場景中發現先前沒有注意到的伏筆，讓你對故事的理解更加完整。觀看網上討論，已經出現不少對細節上各種象徵的解讀，可見製作組的用功，也可見這款遊戲的魅力。\\n四月望雨：典故與老歌巧思妙用\\n《返校》的音樂，除了張衛帆精心編寫的原創樂曲，還引用了1930-40年代鄧雨賢作曲的台語歌《四季紅》、《月夜愁》、《望春風》、《雨夜花》；近人將這四首老歌合稱「四月望雨」，是台語流行歌史必談的作品。\\n劇情中首先出現的，是在匣式錄音帶的第四首（第二首是有點年紀的台灣人都記得的「國民健康操」），一段鋼琴旋律，強制循環播放，我一聽就認出來了：《雨夜花》的第一句！\\n女主角必須走到音樂教室，在鋼琴上彈出這首歌的旋律才能繼續劇情。我立即感到了這首歌所帶來的強烈反諷。可惜台灣以外的玩家可能很少能在第一時間聽出來，我在下面貼出《雨夜花》的歌詞：\\n雨夜花 雨夜花 受風雨吹落地\\n無人看見 每日怨嗟 花謝落土不再回\\n花落土 花落土 有誰人倘看顧\\n無情風雨 誤阮前途 花蕊若落欲如何\\n雨無情 雨無情 無想阮的前程\\n並無看顧 軟弱心性 予阮前途失光明\\n雨水滴 雨水滴 引阮入受難池\\n怎樣予阮 離葉離枝 永遠無人倘看見\\n戒嚴時期，《雨夜花》因幽怨傷感而被列為禁歌，後來到黨外運動時期，它成為了一首譬喻台灣處境、寄託受難者情懷的政治歌曲。在遊戲中，女主角剛彈完這首歌的旋律，音樂教室馬上就出現為你鼓掌的同學，然後教室中央的鳥籠打開，你就可以取得籠中的刑徒戲偶了。\\n這真是極大的諷刺，但又極為合理。現實中，當年的學校裏自然不可能教唱這首禁歌，但也許有人在音樂教室偷偷彈過。又或許，最讓方芮欣縈迴腦際的歌曲，就是這首悲歌。於是在無限輪迴的回憶中，她必須彈奏這段旋律，來想起自己曾經作過什麼孽。\\n返校\\n《返校》遊戲截圖\\n到第三章，場景轉換到方芮欣的家，她的房間裏有一台收音機，玩家調頻時會先後收到三個訊號，聽到三首歌，分別對應三個時段的記憶；《望春風》對應方芮欣父母感情破裂前後，《四季紅》對應她在張明暉老師身上找到寄託前後，《月夜愁》則對應張老師忽然疏遠了她的時候。這段穿梭交待不同時間故事的手法，被許多玩家譽為神筆。\\n《望春風》的歌詞是少女懷春，《四季紅》則是男女對唱打情罵俏，《月夜愁》寫得是被情人冷落的不解與煩愁。三首歌皆採用30年代純純演唱的原版，唯有《雨夜花》沒有使用原版唱片，沒有出現歌詞，而是讓女主角在第四章看到中年魏仲廷返校再在鋼琴上演奏了完整版。\\n雖然可以說劇情編排並不需要把《雨夜花》唱出來，但我覺得沒有出現的《雨夜花》歌詞，或許才是分量最重的，而且人們對這首歌的記憶未必來自原版唱片，更可能是在禁令中的私下傳唱。《雨夜花》約在1970年代後期解禁（參見李坤城：《台灣戒嚴時期禁歌漫談》），後來有不少歌星翻唱，現在聽過原版的人還真不多。\\n返校\\n魏仲廷出獄後寫給自己的一張手札，文辭凝煉深細，排比疏密有致，確實是有故事的人的口吻。\\n在作品裏引用一些老歌，一點都不難，難的是用得恰到好處，並且讓劇情和它成立一種別樣的對比關係，沿着歷史脈絡來造出新意，而赤燭團隊做到了這一點。不僅歌曲，遊戲中的書信，也摹擬了國民黨的官腔、瓊瑤的文藝腔、老一輩文白相間的筆記體，還有現代詩，都寫得傳神。這等文字功底，在遊戲界是非常罕見的，更格外令人欣賞。在詢問後得知，這些文字皆出自1979年生的姚舜庭筆下，但願下一輩的遊戲製作人也能從這裏學到兩手，練就寫文章的好功力。……喺\\n', metadata={'source': 'experiment/texts/foreign/返校：疊合慘白歲月的殘影，窺見戒嚴之下的人心｜端傳媒 Initium Media.txt'})]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# New papers, iterate each paper\n",
    "# newPaper_vector_store = createVectorStore(e_paperNames[7])\n",
    "newPaper_vector_store = createVectorStore_foreign(d_paperNames[5])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-18T00:07:58.512602Z",
     "start_time": "2025-02-18T00:07:57.825531Z"
    }
   },
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "template_foreign = \"\"\"\n",
    "your_research below is written in English while new_paper is in Chinese. Based on your_research and new_paper, answer the question below. Please give me as much details as possible. If you can't answer the question, please explain the reasons in detail. \n",
    "\n",
    "your_research: {context_a}\n",
    "\n",
    "new_paper: {context_b}\n",
    "\n",
    "question: {question}\n",
    "\"\"\"\n",
    "\n",
    "template_english = \"\"\"\n",
    "Based on your_research and new_paper, answer the question below. Please give me as much details as possible. If you can't answer the question, please explain the reasons in detail.\n",
    "\n",
    "your_research: {context_a}\n",
    "\n",
    "new_paper: {context_b}\n",
    "\n",
    "question: {question}\n",
    "\"\"\"\n",
    "# prompt = ChatPromptTemplate.from_template(template_english)\n",
    "prompt = ChatPromptTemplate.from_template(template_foreign)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-18T00:07:05.758006Z",
     "start_time": "2025-02-18T00:07:05.754670Z"
    }
   },
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "chain = (\n",
    "    {\"context_a\": vectorstore_local.as_retriever(), \"context_b\": newPaper_vector_store.as_retriever(), \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | parser\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-02-18T00:08:07.442311Z",
     "start_time": "2025-02-18T00:08:07.438826Z"
    }
   },
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the contents of the new_paper, there are several innovative ideas that could potentially extend the research presented in your_research:\n",
      "\n",
      "1. Integration of Traditional Elements: The new_paper discusses how the game \"返校\" incorporates traditional elements such as old songs, folk beliefs, and historical settings to create a unique and immersive experience for players. This integration of traditional elements could be explored in your_research to analyze how incorporating traditional elements in experimental projects can enhance the overall experience and engagement of the audience.\n",
      "\n",
      "2. Narrative Techniques: The new_paper highlights the use of intertwined narratives in the game \"返校\" where players can discover new details and symbolism upon replaying the game. This narrative technique could be further explored in your_research to investigate how non-linear storytelling or interactive narratives can shape the audience's understanding of the content and lead to a more engaging experience.\n",
      "\n",
      "3. Cultural Impact: The new_paper discusses how the game \"返校\" incorporates cultural references such as old songs and historical events to provide players with a deeper understanding and appreciation of Taiwanese culture. Your research could explore the cultural impact of experimental projects and how incorporating cultural elements can help bridge the gap between different audiences and create a more inclusive experience.\n",
      "\n",
      "4. Creative Use of Music: The new_paper mentions how the game \"返校\" creatively uses music, including old Taiwanese songs, to enhance the atmosphere and storytelling of the game. Your research could delve into the creative use of music in experimental projects and analyze how music can evoke emotions, create ambiance, and enhance the overall experience for the audience.\n",
      "\n",
      "By incorporating these innovative ideas inspired by the new_paper, your_research could potentially explore new avenues in experimental projects, narrative techniques, cultural impact, and creative use of music to further enhance the understanding and appreciation of multimedia content.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# question = \"What is the relationship between your_research and new_paper?\"\n",
    "# question = \"What is the relationship between your_research and new_paper in terms of nonhuman, puppet, avatar, historical trauma, memory, identity, Taiwan, video games, technology, white terror?\"\n",
    "\n",
    "# question = \"What is the relationship between your_research and new_paper in terms of performance?\"\n",
    "\n",
    "question = \"Based on the contents of new_paper, please provide innovative ideas that were not mentioned in your_research that could potentially extend your_research, regarding nonhuman, puppet, avatar, historical trauma, memory, identity, Taiwan, video games, technology, white terror.\"\n",
    "# question = \"Based on comparing your_research and new_paper, please suggest some innovative ideas to help me improve your_research.\"\n",
    "# question = \"Based on comparing your_research and new_paper, please suggest some innovative ideas to help me improve your_research, regarding performance, world, game, detention, people, japanese, political, virtual, historical, human, theatre, space, taiwanese, puppets, media.\"\n",
    "# How can responsible AI be applied to humanities studies?\n",
    "print(chain.invoke(question))\n",
    "# encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "# tokens = [encoding.decode_single_token_bytes(token) for token in encoding.encode(question)]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-05-12T22:34:20.542213Z",
     "start_time": "2024-05-12T22:34:14.661153Z"
    }
   },
   "execution_count": 1085
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-18T00:08:19.820003Z",
     "start_time": "2025-02-18T00:08:11.071195Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the contents of the new_paper, there are several innovative ideas that could potentially extend the research on nonhuman, puppet, avatar, historical trauma, memory, identity, Taiwan, video games, technology, and white terror. Some of these ideas include:\n",
      "\n",
      "1. **Exploration of Moral Dilemmas:** The new_paper discusses how the game \"Detention\" presents players with moral dilemmas and consequences based on their choices. Expanding on this idea, further research could delve into how such moral dilemmas and decision-making processes impact players' perceptions of historical events and trauma. This could involve studying player behavior in response to these dilemmas and the ethical implications of their choices.\n",
      "\n",
      "2. **Interactive Storytelling:** The new_paper highlights how \"Detention\" effectively uses clues and memory fragments to construct a compelling narrative. Research could focus on the impact of interactive storytelling techniques in video games on players' engagement with historical themes and traumatic events. This could involve analyzing how player agency influences the interpretation and understanding of historical trauma within the game's narrative.\n",
      "\n",
      "3. **Virtual Reality and Immersive Experiences:** Building on the immersive experience offered by \"Detention,\" further research could explore the potential of virtual reality technology in enhancing players' engagement with historical trauma and memory. Investigating how virtual reality environments can simulate the atmosphere of historical events, such as the White Terror in Taiwan, could provide valuable insights into the intersection of technology, storytelling, and trauma representation.\n",
      "\n",
      "4. **Comparative Analysis of Narrative Structures:** The new_paper mentions the intricate narrative structure of \"Detention\" and how it gradually reveals the protagonist's story. Extending this research, a comparative analysis could be conducted to examine how different video games approach the representation of historical trauma and memory. Comparing the narrative structures, character development, and player interactions in various games could offer a broader perspective on the role of storytelling in conveying complex historical themes.\n",
      "\n",
      "5. **Cultural and Political Context:** While the new_paper briefly touches on the political context of Taiwan's White Terror, further research could explore the cultural and political influences on the representation of historical trauma in video games. Investigating how developers navigate sensitive historical topics, negotiate political narratives, and engage with diverse cultural perspectives could shed light on the complexities of creating meaningful and respectful portrayals of traumatic events in gaming.\n",
      "\n",
      "By incorporating these innovative ideas into the existing research on puppet, avatar, historical trauma, memory, identity, Taiwan, video games, technology, and white terror, a more comprehensive understanding of how video games can serve as a platform for exploring and addressing complex historical themes can be achieved.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# question = \"What is the relationship between your_research and new_paper?\"\n",
    "# question = \"What is the relationship between your_research and new_paper in terms of nonhuman, puppet, avatar, historical trauma, memory, identity, Taiwan, video games, technology, white terror?\"\n",
    "\n",
    "# question = \"What is the relationship between your_research and new_paper in terms of performance?\"\n",
    "\n",
    "question = \"Based on the contents of new_paper, please provide innovative ideas that were not mentioned in your_research that could potentially extend your_research, regarding nonhuman, puppet, avatar, historical trauma, memory, identity, Taiwan, video games, technology, white terror.\"\n",
    "# question = \"Based on comparing your_research and new_paper, please suggest some innovative ideas to help me improve your_research.\"\n",
    "# question = \"Based on comparing your_research and new_paper, please suggest some innovative ideas to help me improve your_research, regarding performance, world, game, detention, people, japanese, political, virtual, historical, human, theatre, space, taiwanese, puppets, media.\"\n",
    "# How can responsible AI be applied to humanities studies?\n",
    "print(chain.invoke(question))\n",
    "# encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "# tokens = [encoding.decode_single_token_bytes(token) for token in encoding.encode(question)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
